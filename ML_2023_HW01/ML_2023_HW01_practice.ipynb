{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab0f35ce",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484fdd20",
   "metadata": {},
   "source": [
    "To solve this task efficiently, here are some practical suggestions:\n",
    "\n",
    "* You are **HIGHLY RECOMMENDED** to read relevant documentation, e.g. for [python](https://docs.python.org/3/), [numpy](https://docs.scipy.org/doc/numpy/reference/), [matlpotlib](https://matplotlib.org/) and [sklearn](https://scikit-learn.org/stable/). Also remember that tutorials, lecture slides, [Google](http://google.com) and [StackOverflow](https://stackoverflow.com/) are your close friends during this course (and, probably, the whole life?).\n",
    "\n",
    "\n",
    "* Instead of rewriting existing code, if not explicitly asked to do so, use **BUILT-IN METHODS** available in the libraries. There exists a class/method for almost everything you can imagine (related to this homework).\n",
    "\n",
    "\n",
    "* To complete this part of the homework, you have to write some **CODE** directly inside the specified places in the notebook **CELLS**.\n",
    "\n",
    "\n",
    "* In some problems you are asked to provide a short discussion of the results. In these cases you have to create a **MARKDOWN** cell with your comments right after the corresponding code cell.\n",
    "\n",
    "\n",
    "* For every separate problem, you can get **INTERMEDIATE scores**.\n",
    "\n",
    "\n",
    "* Your **SOLUTION** notebook **MUST BE REPRODUCIBLE**, i.e. if a reviewer executes your code, the output will be the same (with all the corresponding plots) as in your uploaded files. For this purpose, we suggest to fix random `seed` or (better) define `random_state=` inside every algorithm that uses some pseudo randomness.\n",
    "\n",
    "\n",
    "* Your code must be readable to any competent reviewer. For this purpose, try to include **necessary** (and not more) comments inside the code. But remember: **GOOD CODE MUST BE SELF-EXPLANATORY**.\n",
    "\n",
    "\n",
    "* Many `sklearn` algorithms support multithreading (Ensemble Methods, Cross-Validation, etc.). Check if the particular algorithm has `n_jobs` parameter and set it to `-1` to use all the cores.\n",
    "\n",
    "\n",
    "* **IMPORTANT:** In the end you need to hand in a **single zip file** containing **two notebooks** (theory and practice) as well as the **html exported versions** of these notebooks. That is **4** files in total.\n",
    "\n",
    "\n",
    "To begin let's import the essential (for this assignment) libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a046d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set default parameters for plots\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2eb140-8932-41b1-995e-1d322c9f7a36",
   "metadata": {},
   "source": [
    "# Least Mean Squares\n",
    "\n",
    "In the **regression** problem objects (feature vectors) $\\mathbf{x}_{1}, \\dots, \\mathbf{x}_{m} \\in \\mathbb{R}^d$ have real-valued labels $y_{1}, \\dots, y_{m} \\in \\mathbb{R}$.\n",
    "\n",
    "In Least Mean Squares the hypothesis function is given by a linear combination of the features $h_{\\theta}(\\mathbf{x}) = \\mathbf{\\theta}^\\top \\mathbf{x}$.\n",
    "\n",
    "The maximum likelihood estimator for the Least Mean Squares problem is the solution of the following optimization problem.\n",
    "\n",
    "$$\n",
    "\\theta^{*} = \\arg \\min_{\\theta} \\|\\mathbf{X}\\theta - \\mathbf{y}\\|^2,\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}$ is a $m\\times d$ matrix that has the feature vectors as rows and $\\mathbf{y} \\in \\mathbb{R}^m$ is a vector consisting of all the labels.\n",
    "\n",
    "This problem is known to have a unique closed-form solution in the case of a full-rank $\\mathbf{X}$. \n",
    "\n",
    "$$\n",
    "\\theta^{*} = (\\mathbf{X}^\\top\\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "In the first part of this assignment you are asked to implement this solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7c059-c921-4205-a2fb-7e8d09d74756",
   "metadata": {},
   "source": [
    "## Task 1: Regressor Class [10 points]\n",
    "\n",
    "In this task you are asked to implement a regressor class that fits the Least Squares parameters to the data and performs predictions at new data points. You need to implement 3 methods: fit, predict and score. `fit` estimates and stores the coefficients of the linear regression in *self.coef_*. `predict` performs prediction using the estimated coefficients. `score` calculates the mean squared error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26aa282-42b5-41ef-beb7-37bd8ae7a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeastSquares(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = None\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param X: array of shape (m, d)\n",
    "        :param y: array of shape (m,)\n",
    "        \"\"\"\n",
    "        ### BEGIN Solution\n",
    "\n",
    "        ### END Solution\n",
    "        \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: array of shape (m, d)\n",
    "        :return: array of shape (m,)\n",
    "        \"\"\"\n",
    "        assert self.coef_ is not None, \"Fit the model first!\"\n",
    "        \n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        ### END Solution\n",
    "        \n",
    "        assert predictions.shape == (X.shape[0],), \"Check shapes!\"\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X: np.ndarray, y: np.ndarray) -> np.float64:\n",
    "        \"\"\"\n",
    "        Calculates mean squared error.\n",
    "        \n",
    "        :param x: array of shape (m, d)\n",
    "        :param y: array of shape (m,)\n",
    "        :return: single number\n",
    "        \"\"\"\n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe51c05-00ee-487e-bf2e-ad3eb0f9472e",
   "metadata": {},
   "source": [
    "## Task 2. Regression Line [10 points]\n",
    "\n",
    "In this task, your goal is to visualize the **regression line** of **Least Mean Squares** applied to a synthetic dataset. In the cell below we generate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55583b0c-bf62-4fd0-9408-b102fd51e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def f(X, noise_rate: float = 0):\n",
    "    return 3 * X**3 - 5 * X + 5 + noise_rate * np.random.randn(X.shape[0])\n",
    "\n",
    "n_samples = 100\n",
    "X = np.linspace(-2, 2, n_samples)\n",
    "y = f(X, noise_rate=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, shuffle=True, random_state=0)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.scatter(X_train, y_train, label=\"Training points\")\n",
    "plt.scatter(X_test, y_test, label=\"Test points\")\n",
    "\n",
    "xrange = np.linspace(-2, 2, 1000)\n",
    "plt.plot(xrange, f(xrange, noise_rate=0), c=\"r\", label=\"True function\")\n",
    "plt.plot()\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c3773-0e3e-436b-a27c-bac337433dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c8713-6d30-4c35-8ac7-fcbd7b3862d1",
   "metadata": {},
   "source": [
    "Fit your model to the training data and visualize the predictions on the same plot with the data points. Report the mean squared error on the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946741aa-dffb-4821-80b2-25e756e1b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4442b00c-6cde-4524-a5b8-14b45aa20267",
   "metadata": {},
   "source": [
    "You may notice that the prediction is far from being accurate. In this example the features are 1-dimensional. So the hypothesis function has the form $\\theta x$, where both $\\theta$ and $x$ are real numbers. Hence, the regression line has to go through the point (0, 0), which leads to inaccuracies, when $y$ is biased, as in the example above. To overcome this, a common practice is to add an additional column of 1s to the data matrix $\\mathbf{X}$. This will model the intercept of the regression line.\n",
    "\n",
    "Implement this and visualize/report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e96fdd7-caa8-4f62-9d33-1279121015e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ae9eb-0990-4890-8d05-d501f7efc30a",
   "metadata": {},
   "source": [
    "# Locally Weighted Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0035d4-2081-4d1e-b1c1-6ce8907eaf63",
   "metadata": {},
   "source": [
    "As you may observe in the previous task, linear models have difficulties with fitting non-linear functions. One way to overcome this could be to consider non-linear modifications of the original features in the model. We will leave this for later. Another way is to do Locally Weighted Regression (LWR). \n",
    "\n",
    "In LWR a separate regression is fitted and used to predict the outcome at each query poins. The training examples are weighted according to their similarity to the query point (in the simplest form, the distance to the query point is used). That is, for a data point $\\mathbf{x}_0 \\in \\mathbb{R}^d$ the prediction is given by:\n",
    "\n",
    "$$\n",
    "\\hat y_0 = \\theta^*(\\mathbf{x}_0)^\\top \\mathbf{x}_0\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\theta^*(\\mathbf{x}_0) = \\arg \\min_{\\theta(\\mathbf{x}_0)} \\sum_{i = 1}^m w^{(i)}(\\mathbf{x}_0) \\left(y_i - \\theta(\\mathbf{x}_0)^\\top \\mathbf{x}_i\\right)^2\n",
    "$$\n",
    "\n",
    "Let $W(\\mathbf{x}_0)$ be a $m\\times m$ diagonal matrix with $w^{(i)}(\\mathbf{x}_0)$ on the $i$-th diagonal element. Then under the same assumptions as for LMS a closed form solution can be derived for $\\hat y_0$.\n",
    "\n",
    "$$\n",
    "\\hat y_0 = \\mathbf{x}_0^\\top (\\mathbf{X}^\\top W(\\mathbf{x}_0) \\mathbf{X})^{-1} X^\\top W(\\mathbf{x}_0) \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6bc4e-4c7f-45be-9847-777eb9004ba9",
   "metadata": {},
   "source": [
    "## Task 3. Implement LWR [15 points]\n",
    "\n",
    "In this task you need to implement a class that preforms Locally Weighted Linear Regression. As the model is fitted again for every new query point, the `fit` method only needs to memorise the dataset. You are asked to implement 3 methods: kernel, predict_ and score. `kernel` constructs the $W(\\mathbf{x}_0)$ matrix. Let's consider a specific kernel here, that is given by:\n",
    "\n",
    "$$\n",
    "w^{(i)}(\\mathbf{x}_0) = \\exp\\left(- \\frac{\\|\\mathbf{x}_0 - \\mathbf{x}_i\\|^2}{2 \\tau^2}\\right)\n",
    "$$\n",
    "\n",
    "`predict_` performs prediction for a single data sample. `score`, as before, calculates the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a06653-41cd-4c26-8702-86459239d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalLeastSquares(object):\n",
    "    def __init__(self, tau: float):\n",
    "        self.tau_ = tau\n",
    "        \n",
    "        self.X_ = None\n",
    "        self.y_ = None\n",
    "        \n",
    "        self.coef_ = None\n",
    "        \n",
    "    def kernel(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param x: array of shape (d,)\n",
    "        :return: array of shape (m, m)\n",
    "        \"\"\"\n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        ### END Solution\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param X: array of shape (m, d)\n",
    "        :param y: array of shape (m,)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "                \n",
    "    def predict_(self, x: np.ndarray) -> np.float64:\n",
    "        \"\"\"\n",
    "        :param x: array of shape (d,)\n",
    "        :return: single number, prediction at x\n",
    "        \"\"\"\n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        ### END Solution\n",
    "        \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param X: array of shape (m, d)\n",
    "        :return: array of shape (m,)\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.array([self.predict_(x) for x in X])\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculates mean squared error.\n",
    "        \n",
    "        :param X: array of shape (m, d)\n",
    "        :param y: array of shape (m,)\n",
    "        :return: single number\n",
    "        \"\"\"\n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e59b5ce-bffb-4466-a52a-64c6c3b83e4d",
   "metadata": {},
   "source": [
    "Visualize the resulting regression line along with the data points and train/test MSE, as done in the previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494889e2-0e01-47fb-82f1-ff91415f248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8778684e-426c-4028-91b0-c66c884b271a",
   "metadata": {},
   "source": [
    "## Task 4. Tune the hyperparameters [7 points]\n",
    "\n",
    "$\\tau$ is an important hyperparameter that can regulate overfitting of the method. Choose an optimal value of $\\tau$ in order to minimize the residual sum of squares on the left out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab010d-e160-4b04-a638-5711083f0349",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da3a12-51e4-4790-bfda-ae65f1237785",
   "metadata": {},
   "source": [
    "What is the effect of different values for $\\tau$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d4c7e-d853-46ec-aea2-d735f139fef9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4430d0a9-8d52-46dc-9cde-7bac97e725b3",
   "metadata": {},
   "source": [
    "What are advantages/disadvantages of the LWR method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c021e-eb84-463a-b01a-9dc42c85bb71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0502626d",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Now let's consider **classification** task. In the **binary classification** problem objects (feature vectors) $\\mathbf{x}_{1}, \\dots, \\mathbf{x}_{m} \\in \\mathbb{R}^d$ have binary labels $y_{1}, \\dots, y_{m} \\in \\{0, 1\\}$.\n",
    "\n",
    "Using a **linear combination** of the features $\\mathbf{\\theta}^\\top \\mathbf{x}$, like in Least Mean Squares, will result in an unbounded estimator. However, we would like to have a mapping $f_{\\mathbf{\\theta}}: \\mathbb{R}^d \\to \\{0, 1\\}$, the output of which can be naturally interpreted as the probability of belonging to class 1.\n",
    "\n",
    "In **Logistic Regression** the resulting dot-product $\\mathbf{\\theta}^\\top \\mathbf{x}$ is converted to the unit interval with the **sigmoid** function:\n",
    "\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "This gives us the hypothesis function:\n",
    "\n",
    "$$f_{\\mathbf{\\theta}}(\\mathbf{x}) = g(\\mathbf{\\theta}^{\\top}\\mathbf{x}) = \\frac{1}{1 + e^{-\\mathbf{\\theta}^{\\top}\\mathbf{x}}}$$\n",
    "\n",
    "Now, we only need to set a **threshold** (for example, 0.5) for classifying an object to the 1st class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecebedd",
   "metadata": {},
   "source": [
    "## Task 5. Sigmoid [4 points]\n",
    "\n",
    "Implement and plot the sigmoid function. \n",
    "\n",
    "**Important!** Your function should work for inputs of arbitrary shape. The sigmoid should be applied elementwise. The returned array should have the same shape as the input. \n",
    "\n",
    "**Important!!** For large negative input, computing the exponent in the sigmoid may result in overflow. Use an alternative form of the sigmoid for the negative entries of the input to deal with this issue.\n",
    "\n",
    "**Hint:** An alternative form can be obtained by multiplying the nominator and the denominator of the sigmoid by $e^z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param z: array of arbitrary shape\n",
    "    \"\"\"\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should not raise a warning\n",
    "sigmoid(np.array([-1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-10, 10, 100)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "### BEGIN Solution\n",
    "\n",
    "### END Solution\n",
    "\n",
    "plt.title('Sigmoid', size=16)\n",
    "\n",
    "plt.xlabel('z', size=14)\n",
    "plt.ylabel('sigmoid(z)', size=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e609491e",
   "metadata": {},
   "source": [
    "Thus, the conditional probabilities of belonging to class 1 or 0 are as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "    p(y = 1| \\mathbf{x}; \\mathbf{\\theta}) &= f_{\\mathbf{\\theta}}(\\mathbf{x}) \\\\\n",
    "    p(y = 0| \\mathbf{x}; \\mathbf{\\theta}) &= 1 - f_{\\mathbf{\\theta}}(\\mathbf{x})\n",
    "\\end{align}$$\n",
    "\n",
    "Or one could rewrite it as:\n",
    "\n",
    "$$p(y| \\mathbf{x}; \\mathbf{\\theta}) = f_{\\mathbf{\\theta}}(\\mathbf{x})^{y}\\bigl[1 - f_{\\mathbf{\\theta}}(\\mathbf{x})\\bigr]^{1 - y}$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Now, to **estimate** the weights $\\mathbf{\\theta}$, we will **maximize** the **likelihood** function (more precisely, its logarithm for simpler calculations). Therefore, this method is called the **maximum likelihood estimation** (MLE).\n",
    "\n",
    "$$\\ln{\\mathcal{L}(\\mathbf{\\theta})} = \\mathcal{l}(\\mathbf{\\theta}) = \\sum\\limits_{i = 1}^m p(y_i| \\mathbf{x}_i; \\mathbf{\\theta}) = \\sum\\limits_{i = 1}^m y_i \\ln{\\bigl[f_{\\mathbf{\\theta}}(\\mathbf{x}_i) \\bigr]} + (1 - y_i)\\ln{\\bigl[1 - f_{\\mathbf{\\theta}}(\\mathbf{x}_i) \\bigr]} \\to \\max\\limits_{\\mathbf{\\theta}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9369a",
   "metadata": {},
   "source": [
    "## Task 6. Log-Likelihood [3 points]\n",
    "\n",
    "Let $\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^{\\top} \\\\ \\vdots \\\\ \\mathbf{x}_m^{\\top} \\end{bmatrix} \\in \\mathbb{R}^{m \\times d}$ be the data matrix, $\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_m \\end{bmatrix} \\in \\mathbb{R}^{m}$ be the labels vector corresponding to $\\mathbf{X}$ and $\\mathbf{\\theta} = \\begin{bmatrix} \\theta_1 \\\\ \\vdots \\\\ \\theta_d \\end{bmatrix} \\in \\mathbb{R}^{d}$ be the parameters vector.\n",
    "\n",
    "Implement the log-likelihood for Logistic Regression.\n",
    "\n",
    "**Note:** You have already implemented the sigmoid function. Now it is time to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_log_reg(theta, X, y):\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98139a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100\n",
    "d = 2\n",
    "\n",
    "X = np.random.randint(low=-10, high=10, size=(m, d))\n",
    "y = np.random.randint(low=0, high=1, size=(m, 1))\n",
    "theta = np.zeros((d, 1))\n",
    "\n",
    "assert np.allclose(log_likelihood_log_reg(theta, X, y),\n",
    "                   -m * np.log(2),\n",
    "                   atol=1e-8), \"Houston, we have a problem!\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5bf54",
   "metadata": {},
   "source": [
    "## Task 7. Gradient Descent [8 points]\n",
    "\n",
    "Now, to find the weights $\\mathbf{\\theta}$, we need to solve the optimization problem described above. Let's use [**gradient descent**](https://en.wikipedia.org/wiki/Gradient_descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203a7e2",
   "metadata": {},
   "source": [
    "Since the gradient descent solves the **minimization** problem, we will change the maximization problem described above to the minimization problem by changing the sign of the log-likelihood function to negative:\n",
    "\n",
    "$$\\mathcal{l}(\\mathbf{\\theta}) \\to \\max\\limits_{\\mathbf{\\theta}} \\Longleftrightarrow -\\mathcal{l}(\\mathbf{\\theta}) \\to \\min\\limits_{\\mathbf{\\theta}}$$\n",
    "\n",
    "So, let's first implement a function to compute the gradient of the **negative** log-likelihood function.\n",
    "\n",
    "The gradient is a column vector that has **the same shape as** $\\mathbf{\\theta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_gradient_log_reg(theta, X, y):\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f48b8e",
   "metadata": {},
   "source": [
    "Now implement the general gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4149b76",
   "metadata": {},
   "outputs": [],
   "source": [
    " def gradient_descent(X, y, gradient, theta_0, alpha=0.01, tolerance=1e-8, max_iters_number=100):\n",
    "    \"\"\"\n",
    "    X: data matrix of shape [m, d]\n",
    "    y: labels of shape [m]\n",
    "    gradient: a function to compute the gradient of the neg. log-likelihood\n",
    "    theta_0: initialization of theta of shape [d]\n",
    "    alpha: learning rate\n",
    "    tolerance: a value to detect convergence (if the norm of the update is smaller that tolerance, terminate)\n",
    "    max_iters_number: maximum number of iterations of the algorithm\n",
    "    \n",
    "    return: the final estimation for theta\n",
    "    \"\"\"\n",
    "    if y.ndim == 1:\n",
    "        y = y[:, np.newaxis]\n",
    "    \n",
    "    if theta_0.ndim == 1:\n",
    "        theta_0 = theta_0[:, np.newaxis]\n",
    "    \n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5b2c17",
   "metadata": {},
   "source": [
    "## Task 8. Classifier [10 points]\n",
    "\n",
    "Let's create our own classifier class and then compare it with the [existing method in the sklearn library](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "We need to implement three principal methods:\n",
    "\n",
    "* `fit` to find the coefficients (weights) $\\mathbf{\\theta}$\n",
    "\n",
    "\n",
    "* `predict` to predict the labels $\\mathbf{\\hat{y}}$ for the data matrix $\\mathbf{X}$\n",
    "\n",
    "\n",
    "* `score` to evaluate predictions (for example, with **mean accuracy** score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b0ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(predictions, labels):\n",
    "    \"\"\"\n",
    "    Returns the accuracy of predictions when compared to the true labels\n",
    "    \"\"\"\n",
    "    assert predictions.shape == labels.shape, \"Check shapes!\"\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        ### END Solution\n",
    "        \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        ### END Solution\n",
    "        \n",
    "        assert predictions.shape == (X.shape[0],), \"Check shapes!\"\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        ### BEGIN Solution\n",
    "        \n",
    "        ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551ddea",
   "metadata": {},
   "source": [
    "## Task 9. Decision Rule [6 points]\n",
    "\n",
    "In this task, your goal is to visualize the **decision rule** of **Logistic Regression** applied to a synthetic $2$-dimensional dataset generated by a built-in `sklearn.datasets` method called `make_moons`. In the cell below we generate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a96692",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=300, noise=0.2, random_state=0)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2d2bd",
   "metadata": {},
   "source": [
    "The goal is to fit our LogReg and sklearn's Logistic Regreesion classifiers to this data:\n",
    "\n",
    "You have to plot the decision regions. The plots must have **titles**, which contain the names of the classifiers and the corresponding accuracy (rounded to only **two** decimal places).\n",
    "\n",
    "You can write the plotting code on your own, but we highly recommend just to use [mlxtend](http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/) library (`pip install mlxtend`), which has a awesome one-line decision boundary plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ceb155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64657e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_logistic_regression = LogReg()\n",
    "logistic_regression = LogisticRegression(random_state=0, solver='lbfgs', n_jobs=-1)\n",
    "\n",
    "classifiers = [my_logistic_regression, logistic_regression]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(16, 8))\n",
    "\n",
    "for classifier, axis in zip(classifiers, axes.flat):\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25318d8d",
   "metadata": {},
   "source": [
    "## Task 10. Feature Engineering [7 points]\n",
    "\n",
    "In the previous task, classifiers obviously failed fitting to data. This happened because the decision boundary of the classifier has a restricted linear form, while the data is much more complicated.\n",
    "\n",
    "One may try to change the parameters of the classifier in order to improve accuracy, but linear models do not have parameters that can change the form of the decision rule.\n",
    "\n",
    "In this case, the **feature engineering** helps: one may try to compute new (e.g. non-linear) features based on the existing ones and fit the classifier for the new features. This may help low-complexity classifiers to fit complex data dependencies.\n",
    "\n",
    "Your task is\n",
    "\n",
    "* to achieve accuracy $> 0.95$, by generating additional features (e.g. polynomial),\n",
    "\n",
    "\n",
    "* to plot decision rules in the original feature space,\n",
    "\n",
    "\n",
    "* to write 2-3 sentences about why you chose these features.\n",
    "\n",
    "It is your choice how to generate features. You may create hand-crafted features and add them manually.\n",
    "\n",
    "Nevertheless, we **highly recommend** getting used to and applying the following built-in `sklearn` methods, for example:\n",
    "\n",
    "* `PolynomialFeatures` for [feature generation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "\n",
    "* `StandardScaler`for [feature scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "* `Pipeline` - for [combining several operations](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) in a row (e.g. feature creation & prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0d2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c78e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_simple = Pipeline([('LR', LogisticRegression(random_state=0, solver='lbfgs', n_jobs=-1))])\n",
    "\n",
    "### BEGIN Solution\n",
    "\n",
    "### END Solution\n",
    "\n",
    "classifiers = [logistic_regression_simple, logistic_regression_advanced]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858d7259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(16, 8))\n",
    "\n",
    "for classifier, axis in zip(classifiers, axes.flat):\n",
    "    ### BEGIN Solution\n",
    "    \n",
    "    ### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff096dd",
   "metadata": {},
   "source": [
    "Why did you choose these features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae2944c",
   "metadata": {},
   "source": [
    "# Face classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293a18b",
   "metadata": {},
   "source": [
    "In this task you will face a real-life problem of face detection. You have to train a model to classify 24$\\times$24 grayscale images to *face*/*non-face* classes.\n",
    "First, let us import some libraries and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set default parameters for plots\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = loadmat('faces.mat')\n",
    "labels = np.squeeze(data['Labels'])\n",
    "labels[labels == -1] = 0  # Want labels in {0, 1}\n",
    "data = data['Data']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b5216",
   "metadata": {},
   "source": [
    "Each datapoint is a 576-dimentional vector that stores pixel intensities of a flattened grayscale image.\n",
    "If carefully reshaped, one can visualize the datapoints as 24$\\times$24 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89247802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "samples_per_class = 10\n",
    "classes = [0, 1]\n",
    "imgs = np.reshape(data, [-1, 24, 24], order='F')\n",
    "\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(np.equal(labels, cls))\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = y * samples_per_class + i + 1\n",
    "        plt.subplot(len(classes), samples_per_class, plt_idx)\n",
    "        plt.imshow(imgs[idx])\n",
    "        plt.axis('off')\n",
    "        plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e5d8e",
   "metadata": {},
   "source": [
    "Now let us split the dataset into train and test. This will allow to assess the ability of our models to generalize to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40963a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3)\n",
    "num_train = X_train.shape[0]\n",
    "num_test = X_test.shape[0]\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb168d9b",
   "metadata": {},
   "source": [
    "## Task 11. Logistic Regression for Face Classification [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee902403",
   "metadata": {},
   "source": [
    "Now fit your *LogReg* class on *(X_train, y_train)* and report the accuracy on both the **train** and **test** sets.\n",
    "\n",
    "**Warning:** It may take time to fit your model to this amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1dc876-9d21-4ad9-b955-bfd5c4a4a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03064d",
   "metadata": {},
   "source": [
    "Visualize the learned coefficients as a grayscale image. Reshape the coefficients and use *plt.imshow()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f167905",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe18b3",
   "metadata": {},
   "source": [
    "## Task 12. Tuning the model [5 points]\n",
    "\n",
    "The final task is to build a model that reaches as high accuracy on the test set as possible.\n",
    "Feel free to use anything that you already know from the lectures/tutorials/this assignment by the time this assignment was given (i.e. the 4th week).\n",
    "\n",
    "Justify your final choice of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad1f53-2f0f-4b1e-8ed3-9d29a6e3206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
